<!DOCTYPE html>
<html lang="zh-cn">
<head>
    <title>Logistic Regressive笔记</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="content-type" content="text/html; charset=UTF-8" />
    <!-- 最新 Bootstrap 核心 CSS 文件 boostrap 3.0.3-->
    <link rel="stylesheet" href="css/lib/bootstrap.min.css">
    <link rel="stylesheet" href="css/lib/bootstrap-theme.min.css">
    <!-- 本地CSS文件 -->
    <link rel="stylesheet" href="css/mystyle.css">
</head>
<body>

<xmp theme="cerulean" style="display:none;">

### 0. 二分类问题

最简单的讲线性回归扩展为分类问题的模型是“感知机模型”，该模型使用 $y=sign(\theta^Tx)$ 作为判别输出。
该算法实际是后续神经网络模型的基础，但由于其模型过于简单，导致其应用很少。


### 1. Logistic Regressive 模型

#### 1.1 模型说明

首先约定，样本中y的取值0: negative class; 1: positive class. Logistic回归的模型如下：

$$
h_{\theta}(x) = g(\theta^Tx) = \frac{1}{1+exp(-\theta^Tx)}
$$

其中$g(z)$函数为sigmoid函数：

$$
g(z) = \frac{1}{1+exp(-z)}
$$

sigmoid当输入为正无穷的时候，输出接近1，相反输出接近0，其函数图像为：

![sigmoid](img/sigmoid.png)

sigmoid函数有个特点，其导数跟原始函数很接近:

$$
g'(z) = g(z)(1 − g(z))
$$

在Logistic回归模型中，$h_{\theta}(x)$ 返回的是 $p(y=1|x)$, 即:

$$
p(y=1|x) = h_{\theta}(x),  p(y=0|x) = 1 - h_{\theta}(x)
$$

由于我们前面的约定，y取值为0，1，上式可以修改为：

$$
p(y|x) = (h_{\theta}(x))^y (1-h_{\theta}(x))^{(1-y)}
$$


有了上面的模型我们可以通过计算maximize likelihood，获得最终参数$\theta$, 由于IID的假设, 其中Likehood函数：

$$
L(\theta) = \prod_{i=1}^{m} p(y|x) = \prod_{i=1}^{m} (h_{\theta}(x^{(i)}))^{y^{(i)}} (1-h_{\theta}(x^{(i)}))^{(1-y)^{(i)}}
$$


#### 1.2 算法

为了得到最大化likehood函数的参数$\theta$, 通过对数化和简化可以得到：

$$
l(\theta) = log L(\theta) = \sum_{i=1}^{m} y^{(i)} log(h_{\theta}(x^{(i)})) + (1-y^{(i)}) log(1-h_{\theta}(x^{(i)}))
$$

利用梯度下降算法，可以求解出最大化上式的参数$\theta$ , 其中$\theta$更新函数为：

$$
\theta_j := \theta_j + \alpha (y^{(i)}-h_{\theta}(x^{(i)})) x_j^{(i)}
$$

### 2 正则化

为了控制模型的复杂度，减少Overfit的可能性，降低模型的VC维，我们可以加入模型复杂度的惩罚项，对于Logistic模型来说，可以将上面的对数化的优化目标函数加上一个惩罚项目。

$$
l(\theta) = log L(\theta) = \sum_{i=1}^{m} y^{(i)} log(h_{\theta}(x^{(i)})) + (1-y^{(i)}) log(1-h_{\theta}(x^{(i)})) + \frac{1}{2}\lambda \sum_{j=1}^{n}\theta_j^2
$$

在引入了$\lambda$参数后，梯度下降算法中的参数更新方法修改为：
$$
\theta_j := \theta_j + \alpha (y^{(i)}-h_{\theta}(x^{(i)})) x_j^{(i)} + \lambda\theta_j
$$


### 3 Practice

<div class="panel">
    <div class="btn-toolbar" role="toolbar">
      <div class="btn-group" role="group">
        <button type="button" class="btn btn-warning" id="btnRest1">重置(1阶)</button>
        <button type="button" class="btn btn-warning" id="btnRest2">重置(2阶)</button>
        <button type="button" class="btn btn-warning" id="btnRest3">重置(7阶)</button>
        <button type="button" class="btn btn-warning" id="btnRest4">重置(7阶＋正则化)</button>
      </div>
      <div class="btn-group" role="group">
        <button type="button" class="btn btn-info" id="btnLabel0">负样本(0)</button>
        <button type="button" class="btn btn-info" id="btnLabel1">正样本(1)</button>
      </div>
      <div class="btn-group" role="group">
        <button type="button" class="btn btn-primary" id="btnRegressive">回归计算</button>
      </div>
    </div>
    <div class="well" style="margin-top:5px;">
      <div id="dataFrame" style="width:512px;height:512px:margin:0px">
      </div>
    </div>
</div>


### 4 多分类问题，softmax

假设存在k个分类，即y取值为0,1,2,...k-1, 还是采用线性模型$\theta^Tx$，对每一组分类设计一个$\theta_{(i)}$.
将每个分类的线性输出指数化，即每一个分类对应一个输出：$e^{(\theta_{(i)}^Tx)}$，


那么softmax是将某个分类的输出占比，作为该分类的概率估计即：

$$
p(y=i|x;\theta) = \frac{e^{(\theta_{(i)}^Tx)}}{\sum_{j=1}^{k}e^{\theta_{(j)}^Tx}}
$$

因此对数化的最大似然函数为：

$$
l(\theta) = \sum_{i=1}^{m} log \frac{e^{(\theta_{(y^{(i)})}^Tx)}}{\sum_{j=1}^{k}e^{\theta_{(j)}^Tx}}
$$

同样可以采用梯度下降法来确定多组$\theta_{(i)}$


如果我们仔细观察2分类问题的Logistic可以看作一类输出为1(即线性输出为0)，一类输出为$e^{(\theta^Tx)}$，按照softmax方式计算概率。


</xmp>


<!-- jQuery文件。务必在bootstrap.min.js 之前引入 Jquery 1.10.2, bootstrap 3.0.3-->
<script src="js/lib/jquery.min.js"></script>
<script src="js/lib/bootstrap.min.js"></script>
<script src="js/lib/d3.v3.min.js"></script>
<script src="vendor/strapdown/strapdown.min.js"></script>
<script src="js/elj/elj_top.js"></script>
<script src="js/elj/elj_util.js"></script>
<script src="js/elj/elj_gd.js"></script>
<script src="js/elj/elj_logistic.js"></script>
<script src="js/logistic_regressive.js"></script>


<!-- 主函数 -->
<script type="text/javascript">
var guiInit = function() {

  var w = 512,
      h = 512;


  svg = d3.select("#dataFrame").append("svg")
      .attr("width",  w)
      .attr("height", h);

  xScale = d3.scale.linear().domain([-1, 1]).range([0, 512]);
  yScale = d3.scale.linear().domain([1, -1]).range([0, 512]);

  var xAxis = d3.svg.axis().scale(xScale).orient("bottom"),
      yAxis = d3.svg.axis().scale(yScale).orient("left");

  svg.append("g")
      .attr("class", "axis")
      .attr("transform", "translate(-1, 255)")
      .call(xAxis);
  svg.append("g")
      .attr("class", "axis")
      .attr("transform", "translate(255, -1)")
      .call(yAxis);


  svg.append("rect")
     .attr("x", w - 18)
     .attr("y", 20)
     .attr("width", 18)
     .attr("height", 18)
     .style('fill', 'red');
  svg.append("text")
     .attr("x", w - 72)
     .attr("y", 30)
     .text('False: 0');

  svg.append("rect")
     .attr("x", w - 18)
     .attr("y", 40)
     .attr("width", 18)
     .attr("height", 18)
     .style('fill', 'green');
  svg.append("text")
     .attr("x", w - 72)
     .attr("y", 50)
     .text('True: 1');

  // 事件
  document.getElementById("dataFrame").addEventListener('click', ui.onFrameClick, false);
  $("#btnRest1").click(ui.onReset(1));
  $("#btnRest2").click(ui.onReset(2));
  $("#btnRest3").click(ui.onReset(7));
  $("#btnRest4").click(ui.onReset(7, 0.0001));

  $("#btnLabel0").click(ui.onLabel(0));
  $("#btnLabel1").click(ui.onLabel(1));

  $("#btnRegressive").click(ui.onRegressive);
}

var samples = undefined;
var currentLabel = 0;
var currentExtend = 1;
var currentLambda = 0;
var ui = {};

ui.onReset = function(d, lambda) {
  return function() {
    currentExtend = d;
    if ( lambda === undefined) {
      currentLambda = 0;
    } else {
      currentLambda = lambda;
    }

    svg.selectAll("circle").remove();

    samples = elj.util.zeroSamples(2, 2);  // 2分类；2输入空间

  };
};

ui.onLabel = function(l) {
  return function() {
    currentLabel = l;
  };
};


ui.onRegressive = function() {
  var localSamples = elj.util.copySamples(samples);
  if ( currentExtend > 1) {
    elj.util.expandFeatures(localSamples, currentExtend);
  }

  var logistic = new elj.Logistic(localSamples, currentLambda);
  var sgd = new elj.SGD(0.5);
  for(var i = 0; i < 10000; i++) {
    sgd.train(logistic);
  }

  //得到模型值之后，计算分界线
  var coverSamples = elj.util.zeroSamples(2,2);

  for (var x = -1.0; x < 1.0; x += 0.01 ) {
      for (var y = -1.0; y < 1.0; y += 0.01 ) {
          var d = [];
          d.push(-1);
          d.push(x);
          d.push(y);
          coverSamples.d.push(d);
      }
  }
  if ( currentExtend > 1) {
    elj.util.expandFeatures( coverSamples, currentExtend);
  }

  for (var i = 0; i < coverSamples.d.length; i++) {
      var ret = logistic.pred( coverSamples.d[i] );
      if ( Math.abs(ret-0.5) < 0.01 ) {

          var xx = xScale( coverSamples.d[i][1] );
          var yy = yScale( coverSamples.d[i][2] );

          svg.append("circle")
            .attr("class", "dot")
            .attr("r", 1.0)
            .attr("cx", xx)
            .attr("cy", yy);
      }
  }

};


ui.onFrameClick = function(evt) {
  var rect = this.getBoundingClientRect();
  var x = evt.clientX - rect.left;
  var y = evt.clientY - rect.top;

  var xx = xScale.invert(x);
  var yy = yScale.invert(y);

  var circle = svg.append("circle")
        .attr("class", "dot")
        .attr("r", 3.5)
        .attr("cx", x)
        .attr("cy", y)
        .style("fill", currentLabel === 1 ?"lightGreen":"red");

  samples.d.push([currentLabel, xx, yy]);

};



<!-- 主函数 -->
$(document).ready(function() {
  guiInit();
  samples = elj.util.zeroSamples(2, 2);

});
</script>
</body>
</html>
